{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to Calculate Feature Importance With Python\n",
    "link: https://machinelearningmastery.com/calculate-feature-importance-with-python/\n",
    "\n",
    "Feature importance refers to a class of techniques for assigning scores to input features to a predictive model that indicates the relative importance of each feature when making a prediction.\n",
    "\n",
    "Feature importance scores can be calculated for problems that involve predicting a numerical value, called regression, and those problems that involve predicting a class label, called classification.\n",
    "\n",
    "The scores are useful and can be used in a range of situations in a predictive modeling problem, such as:\n",
    "\n",
    "Better understanding the data.\n",
    "Better understanding a model.\n",
    "Reducing the number of input features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coefficients as Feature Importance\n",
    "Linear machine learning algorithms fit a model where the prediction is the weighted sum of the input values.\n",
    "\n",
    "Examples include linear regression, logistic regression, and extensions that add regularization, such as ridge regression and the elastic net.\n",
    "\n",
    "All of these algorithms find a set of coefficients to use in the weighted sum in order to make a prediction. These coefficients can be used directly as a crude type of feature importance score.\n",
    "\n",
    "Letâ€™s take a closer look at using coefficients as feature importance for classification and regression. We will fit a model on the dataset to find the coefficients, then summarize the importance scores for each input feature and finally create a bar chart to get an idea of the relative importance of the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature: 0, Score: 0.16320\n",
      "Feature: 1, Score: -0.64301\n",
      "Feature: 2, Score: 0.48497\n",
      "Feature: 3, Score: -0.46190\n",
      "Feature: 4, Score: 0.18432\n",
      "Feature: 5, Score: -0.11978\n",
      "Feature: 6, Score: -0.40602\n",
      "Feature: 7, Score: 0.03772\n",
      "Feature: 8, Score: -0.51785\n",
      "Feature: 9, Score: 0.26540\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from matplotlib import pyplot\n",
    "# define dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=10, n_informative=5, n_redundant=5, random_state=1)\n",
    "# define the model\n",
    "model = LogisticRegression()\n",
    "# fit the model\n",
    "model.fit(X, y)\n",
    "# get importance\n",
    "importance = model.coef_[0]\n",
    "# summarize feature importance\n",
    "for i,v in enumerate(importance):\n",
    "\tprint('Feature: %0d, Score: %.5f' % (i,v))\n",
    "# plot feature importance\n",
    "pyplot.bar([x for x in range(len(importance))], importance)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection with Importance\n",
    "Feature importance scores can be used to help interpret the data, but they can also be used directly to help rank and select features that are most useful to a predictive model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 84.55\n"
     ]
    }
   ],
   "source": [
    "# evaluation of a model using 5 features chosen with random forest importance\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    " \n",
    "# feature selection\n",
    "def select_features(X_train, y_train, X_test):\n",
    "\t# configure to select a subset of features\n",
    "\tfs = SelectFromModel(RandomForestClassifier(n_estimators=1000), max_features=5)\n",
    "\t# learn relationship from training data\n",
    "\tfs.fit(X_train, y_train)\n",
    "\t# transform train input data\n",
    "\tX_train_fs = fs.transform(X_train)\n",
    "\t# transform test input data\n",
    "\tX_test_fs = fs.transform(X_test)\n",
    "\treturn X_train_fs, X_test_fs, fs\n",
    " \n",
    "# define the dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=10, n_informative=5, n_redundant=5, random_state=1)\n",
    "# split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)\n",
    "# feature selection\n",
    "X_train_fs, X_test_fs, fs = select_features(X_train, y_train, X_test)\n",
    "# fit the model\n",
    "model = LogisticRegression(solver='liblinear')\n",
    "model.fit(X_train_fs, y_train)\n",
    "# evaluate the model\n",
    "yhat = model.predict(X_test_fs)\n",
    "# evaluate predictions\n",
    "accuracy = accuracy_score(y_test, yhat)\n",
    "print('Accuracy: %.2f' % (accuracy*100))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "19985ac8a88737d3c8b7fbc1bc9ac2991a55fab1b7ef4317ae756c7f86ac40fa"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
