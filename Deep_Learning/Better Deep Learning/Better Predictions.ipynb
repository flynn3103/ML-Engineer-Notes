{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reduce Model Variance with Ensemble Learning\n",
    "https://machinelearningmastery.com/ensemble-methods-for-deep-learning-neural-networks/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training data can be varied by fitting models on different subsamples of the dataset. This\n",
    "might involve fitting and retaining models on different randomly selected subsets of the training\n",
    "dataset, retaining models for each fold in a k-fold cross-validation, or retaining models across\n",
    "different samples with replacement using the bootstrap method (e.g. bootstrap aggregation).\n",
    "Collectively, we can think of these methods as resampling ensembles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit Models on Different Samples With Resampling Ensembles\n",
    "https://machinelearningmastery.com/how-to-create-a-random-split-cross-validation-and-bagging-ensemble-for-deep-learning-in-keras/\n",
    "\n",
    "*Ensemble of models fit on different samples of the training\n",
    "dataset*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perhaps the simplest way to vary the members of the ensemble involves gathering models\n",
    "from multiple runs of the learning algorithm on the training dataset. The stochastic learning\n",
    "algorithm will cause a slightly different fit on each run that, in turn, will have a slightly different\n",
    "fit. Averaging the models across multiple runs will ensure the performance remains consistent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine Models From Multiple Runs with Model Averaging Ensemble\n",
    "https://machinelearningmastery.com/model-averaging-ensemble-for-deep-learning-neural-networks/\n",
    "\n",
    "*Retrain models across multiple runs of the same learning\n",
    "algorithm on the same dataset*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variations on this approach may involve training models with different hyperparameter\n",
    "configurations. It can be expensive to train multiple final deep learning models, especially\n",
    "when one model may take days or weeks to fit. An alternative is to collect models for use as\n",
    "contributing ensemble members during a single training run; for example:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models from Contiguous Epochs With Horizontal Voting Ensembles\n",
    "https://machinelearningmastery.com/horizontal-voting-ensemble/\n",
    "\n",
    "*Ensemble members collected from a contiguous block of training\n",
    "epochs towards the end of a single training run*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cyclic Learning Rate and Snapshot Ensembles\n",
    "https://machinelearningmastery.com/snapshot-ensemble-deep-learning-neural-network/\n",
    "\n",
    "*A training run using an aggressive cyclic learning rate where\n",
    "ensemble members are collected at the trough of each cycle of the learning rate*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplest way to combine the predictions from multiple ensemble members is to calculate\n",
    "the average of the predictions in the case of regression, or the statistical mode (most frequent\n",
    "prediction) in the case of classification. Alternately, the best way to combine the predictions\n",
    "from multiple models can be learned; for example:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contribute Proportional to Trust with Weighted Average Ensemble\n",
    "https://machinelearningmastery.com/weighted-average-ensemble-for-deep-learning-neural-networks/\n",
    "\n",
    "*The contribution from each ensemble\n",
    "member to an ensemble prediction is weighted using learned coefficients that indicates the\n",
    "trust in each model*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learn to Combine Predictions With Stacked Generalization Ensemble\n",
    "https://machinelearningmastery.com/stacking-ensemble-for-deep-learning-neural-networks/\n",
    "\n",
    "*A new model is trained to learn how to best\n",
    "combine the predictions from the ensemble members*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An alternative to combining the predictions from the ensemble members, the models them-\n",
    "selves may be combined; for example:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine Model Parameters With Average Model Weights Ensemble\n",
    "https://machinelearningmastery.com/polyak-neural-network-model-weight-ensemble/\n",
    "\n",
    "*Weights from multiple neural network models are\n",
    "averaged into a single model used to make a prediction*"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
