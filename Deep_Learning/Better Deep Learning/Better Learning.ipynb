{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge of Training Deep Learning\n",
    "https://machinelearningmastery.com/a-gentle-introduction-to-the-challenge-of-training-deep-learning-neural-network-models/\n",
    "\n",
    "- **Neural Nets Learn a Mapping Function**: \n",
    "    - A true mapping function exists to best map input variables to output variables and that a neural network model can do a reasonable job at approximating the true unknown underlying mapping function\n",
    "    - We can describe the broader problem that neural networks solve as “function approximation.” They learn to approximate an unknown underlying mapping function given a training dataset. They do this by learning weights and the model parameters, given a specific network structure that we design.\n",
    "\n",
    "- **Learning Network Weights Is Hard**:\n",
    "    - Finding parameters for many machine learning algorithms involves solving a convex optimization problem: that is an error surface that is shaped like a bowl with a single best solution. *This is not the case for deep learning neural networks.*\n",
    "    - It is not a simple bowl shape with a single best set of weights that we are guaranteed to find. Instead, there is a landscape of peaks and valleys with many good and many misleadingly good sets of parameters that we may discover.\n",
    "\n",
    "- **Navigating the Non-Convex Error Surface**: \n",
    "    - Neural network models can be thought to learn by navigating a non-convex error surface.\n",
    "    - Backpropagation refers to a technique from calculus to calculate the derivative (e.g. the slope or the gradient) of the model error for specific model parameters, allowing model weights to be updated to move down the gradient.\n",
    "    - Stochastic gradient descent can be used to find the parameters for other machine learning algorithms, such as linear regression, and it is used when working with very large datasets, although if there are sufficient resources, then convex-based optimization algorithms are significantly more efficient.\n",
    "- **Components of the Learning Algorithm**:\n",
    "    - Loss Function. The function used to estimate the performance of a model with a specific set of weights on examples from the training dataset.\n",
    "    - Weight Initialization. The procedure by which the initial small random values are assigned to model weights at the beginning of the training process.\n",
    "    - Batch Size. The number of examples used to estimate the error gradient before updating the model parameters.\n",
    "    - Learning Rate: The amount that each model parameter is updated per cycle of the learning algorithm.\n",
    "    - Epochs. The number of complete passes through the training dataset before the training process is terminated.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configure Capacity With Nodes and Layers\n",
    "https://machinelearningmastery.com/how-to-control-neural-network-model-capacity-with-nodes-and-layers/\n",
    "\n",
    "*Including including the number of nodes in a layer and the number\n",
    "of layers used to define the scope of functions that can be learned by the model*\n",
    "\n",
    "- Neural network model capacity is controlled both by the number of nodes and the number of layers in the model.\n",
    "- A model with a single hidden layer and a sufficient number of nodes has the capability of learning any mapping function, but the chosen learning algorithm may or may not be able to realize this capability.\n",
    "- Increasing the number of layers provides a short-cut to increasing the capacity of the model with fewer resources, and modern techniques allow learning algorithms to successfully train deep models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# study of mlp learning curves given different number of nodes for multi-class classification\n",
    "from sklearn.datasets import make_blobs\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import to_categorical\n",
    "from matplotlib import pyplot\n",
    " \n",
    "# prepare multi-class classification dataset\n",
    "def create_dataset():\n",
    "\t# generate 2d classification dataset\n",
    "\tX, y = make_blobs(n_samples=1000, centers=20, n_features=100, cluster_std=2, random_state=2)\n",
    "\t# one hot encode output variable\n",
    "\ty = to_categorical(y)\n",
    "\t# split into train and test\n",
    "\tn_train = 500\n",
    "\ttrainX, testX = X[:n_train, :], X[n_train:, :]\n",
    "\ttrainy, testy = y[:n_train], y[n_train:]\n",
    "\treturn trainX, trainy, testX, testy\n",
    " \n",
    "# fit model with given number of nodes, returns test set accuracy\n",
    "def evaluate_model(n_nodes, trainX, trainy, testX, testy):\n",
    "\t# configure the model based on the data\n",
    "\tn_input, n_classes = trainX.shape[1], testy.shape[1]\n",
    "\t# define model\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Dense(n_nodes, input_dim=n_input, activation='relu', kernel_initializer='he_uniform'))\n",
    "\tmodel.add(Dense(n_classes, activation='softmax'))\n",
    "\t# compile model\n",
    "\topt = SGD(lr=0.01, momentum=0.9)\n",
    "\tmodel.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "\t# fit model on train set\n",
    "\thistory = model.fit(trainX, trainy, epochs=100, verbose=0)\n",
    "\t# evaluate model on test set\n",
    "\t_, test_acc = model.evaluate(testX, testy, verbose=0)\n",
    "\treturn history, test_acc\n",
    " \n",
    "# prepare dataset\n",
    "trainX, trainy, testX, testy = create_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change Model Capacity With Nodes\n",
    "\n",
    "We can see that as the number of nodes is increased, the model is able to better decrease the loss, e.g. to better learn the training dataset. This plot shows the direct relationship between model capacity, as defined by the number of nodes in the hidden layer and the model’s ability to learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate model and plot learning curve with given number of nodes\n",
    "num_nodes = [1, 2, 3, 4, 5, 6, 7]\n",
    "for n_nodes in num_nodes:\n",
    "\t# evaluate model with a given number of nodes\n",
    "\thistory, result = evaluate_model(n_nodes, trainX, trainy, testX, testy)\n",
    "\t# summarize final test set accuracy\n",
    "\tprint('nodes=%d: %.3f' % (n_nodes, result))\n",
    "\t# plot learning curve\n",
    "\tpyplot.plot(history.history['loss'], label=str(n_nodes))\n",
    "# show the plot\n",
    "pyplot.legend()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change Model Capacity With Layers\n",
    "- Increasing the number of layers can often greatly increase the capacity of the model, acting like a computational and learning shortcut to modeling a problem.\n",
    "- The danger is that a model with more capacity than is required is likely to overfit the training data, and as with a model that has too many nodes, a model with too many layers will likely be unable to learn the training dataset, getting lost or stuck during the optimization process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate model and plot learning curve of model with given number of layers\n",
    "all_history = list()\n",
    "num_layers = [1, 2, 3, 4, 5]\n",
    "for n_layers in num_layers:\n",
    "\t# evaluate model with a given number of layers\n",
    "\thistory, result = evaluate_model(n_layers, trainX, trainy, testX, testy)\n",
    "\tprint('layers=%d: %.3f' % (n_layers, result))\n",
    "\t# plot learning curve\n",
    "\tpyplot.plot(history.history['loss'], label=str(n_layers))\n",
    "pyplot.legend()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch and an Epoch\n",
    "https://machinelearningmastery.com/difference-between-a-batch-and-an-epoch/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **What Is a Sample?**: A sample is a single row of data. It contains inputs that are fed into the algorithm and an output that is used to compare to the prediction and calculate an error.\n",
    "- **What Is a Batch?**: The batch size is a hyperparameter that defines the number of samples to work through before updating the internal model parameters.\n",
    "    - Batch Gradient Descent. Batch Size = Size of Training Set\n",
    "    - Stochastic Gradient Descent. Batch Size = 1\n",
    "    - Mini-Batch Gradient Descent. 1 < Batch Size < Size of Training Set\n",
    "\n",
    "- **What Is a Epoch?**: The number of epochs is a hyperparameter that defines the number times that the learning algorithm will work through the entire training dataset.\n",
    "- **What Is the Difference Between Batch and Epoch?**: \n",
    "    - The batch size is a number of samples processed before the model is updated.\n",
    "    - The number of epochs is the number of complete passes through the training dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Control the Stability With Batch Size\n",
    "https://machinelearningmastery.com/how-to-control-the-speed-and-stability-of-training-neural-networks-with-gradient-descent-batch-size/\n",
    "\n",
    "*Including exploring whether variations such as batch, stochastic\n",
    "(online), or minibatch gradient descent are more appropriate*\n",
    "- Batch size controls the accuracy of the estimate of the error gradient when training neural networks.\n",
    "- Batch, Stochastic, and Minibatch gradient descent are the three main flavors of the learning algorithm.\n",
    "- There is a tension between batch size and the speed and stability of the learning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mlp for the blobs problem with minibatch gradient descent with varied batch size\n",
    "from sklearn.datasets import make_blobs\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import to_categorical\n",
    "from matplotlib import pyplot\n",
    " \n",
    "# prepare train and test dataset\n",
    "def prepare_data():\n",
    "\t# generate 2d classification dataset\n",
    "\tX, y = make_blobs(n_samples=1000, centers=3, n_features=2, cluster_std=2, random_state=2)\n",
    "\t# one hot encode output variable\n",
    "\ty = to_categorical(y)\n",
    "\t# split into train and test\n",
    "\tn_train = 500\n",
    "\ttrainX, testX = X[:n_train, :], X[n_train:, :]\n",
    "\ttrainy, testy = y[:n_train], y[n_train:]\n",
    "\treturn trainX, trainy, testX, testy\n",
    " \n",
    "# fit a model and plot learning curve\n",
    "def fit_model(trainX, trainy, testX, testy, n_batch):\n",
    "\t# define model\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Dense(50, input_dim=2, activation='relu', kernel_initializer='he_uniform'))\n",
    "\tmodel.add(Dense(3, activation='softmax'))\n",
    "\t# compile model\n",
    "\topt = SGD(lr=0.01, momentum=0.9)\n",
    "\tmodel.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "\t# fit model\n",
    "\thistory = model.fit(trainX, trainy, validation_data=(testX, testy), epochs=200, verbose=0, batch_size=n_batch)\n",
    "\t# plot learning curves\n",
    "\tpyplot.plot(history.history['accuracy'], label='train')\n",
    "\tpyplot.plot(history.history['val_accuracy'], label='test')\n",
    "\tpyplot.title('batch='+str(n_batch), pad=-40)\n",
    " \n",
    "# prepare dataset\n",
    "trainX, trainy, testX, testy = prepare_data()\n",
    "# create learning curves for different batch sizes\n",
    "batch_sizes = [4, 8, 16, 32, 64, 128, 256, 450]\n",
    "for i in range(len(batch_sizes)):\n",
    "\t# determine the plot number\n",
    "\tplot_no = 420 + (i+1)\n",
    "\tpyplot.subplot(plot_no)\n",
    "\t# fit model and plot learning curves for a batch size\n",
    "\tfit_model(trainX, trainy, testX, testy, batch_sizes[i])\n",
    "# show learning curves\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss and Loss Functions\n",
    "https://machinelearningmastery.com/loss-and-loss-functions-for-training-deep-learning-neural-networks/\n",
    "\n",
    "*Including understanding the way different loss functions\n",
    "must be interpreted and whether an alternate loss function would be appropriate for your\n",
    "problem*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configure What to Optimize With Loss Functions\n",
    "https://machinelearningmastery.com/how-to-choose-loss-functions-when-training-deep-learning-neural-networks/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understand the Impact of Learning Rate\n",
    "https://machinelearningmastery.com/understand-the-dynamics-of-learning-rate-on-deep-learning-neural-networks/\n",
    "\n",
    "*Including understanding the effect of different learning rates\n",
    "on your problem and whether modern adaptive learning rate methods such as Adam would\n",
    "be appropriate*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configure Speed of Learning With Learning Rate\n",
    "https://machinelearningmastery.com/learning-rate-for-deep-learning-neural-networks/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stabilize Learning With Data Scaling\n",
    "https://machinelearningmastery.com/how-to-improve-neural-network-stability-and-modeling-performance-with-data-scaling/\n",
    "\n",
    "*Including the sensitivity that small network weights have to\n",
    "the scale of input variables and the impact of large errors in the target variable have on\n",
    "weight updates*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to the Rectified Linear Unit (ReLU)\n",
    "https://machinelearningmastery.com/rectified-linear-activation-function-for-deep-learning-neural-networks/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fix Vanishing Gradients With ReLU\n",
    "https://machinelearningmastery.com/how-to-fix-vanishing-gradients-using-the-rectified-linear-activation-function/\n",
    "\n",
    "*Prevent the training of deep multiple-layered networks causing\n",
    "layers close to the input layer to not have their weights updated; that can be addressed using\n",
    "modern activation functions such as the rectified linear activation function*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fix Exploding Gradients With Gradient Clipping\n",
    "https://machinelearningmastery.com/how-to-avoid-exploding-gradients-in-neural-networks-with-gradient-clipping/\n",
    "\n",
    "*Large weight updates cause a numerical overflow or underflow\n",
    "making the network weights take on a NaN or Inf value; that can be addressed using\n",
    "gradient scaling or gradient clipping*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Batch Normalization\n",
    "https://machinelearningmastery.com/batch-normalization-for-training-of-deep-neural-networks/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accelerate Learning With Batch Normalization\n",
    "https://machinelearningmastery.com/how-to-accelerate-learning-of-deep-neural-networks-with-batch-normalization/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deeper Models With Greedy Layer-Wise Pretraining\n",
    "https://machinelearningmastery.com/greedy-layer-wise-pretraining-tutorial/\n",
    "\n",
    "*Where layers are added one at a time to a model,\n",
    "learning to interpret the output of prior layers and permitting the development of much\n",
    "deeper models: a milestone technique in the field of deep learning*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jump-Start Training With Transfer Learning\n",
    "https://machinelearningmastery.com/how-to-improve-performance-with-transfer-learning-for-deep-learning-neural-networks/\n",
    "\n",
    "*Where a model is trained on a different, but somehow related,\n",
    "predictive modeling problem and then used to seed the weights or used wholesale as a\n",
    "feature extraction model to provide input to a model trained on the problem of interest*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adam Optimization Algorithm\n",
    "https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
