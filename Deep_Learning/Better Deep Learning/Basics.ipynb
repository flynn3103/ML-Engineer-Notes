{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Framework for Better Deep Learning\n",
    "https://machinelearningmastery.com/framework-for-better-deep-learning/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problems of poor performance\n",
    "There are three types of problems that\n",
    "are straightforward to diagnose with regard to poor performance of a deep learning neural\n",
    "network model; they are:\n",
    "- *Problems with Learning*. Problems with learning manifest in a model that cannot\n",
    "effectively learn a training dataset or shows slow progress or bad performance when learning the training dataset.\n",
    "- *Problems with Generalization*. Problems with generalization manifest in a model\n",
    "that overfits the training dataset and makes poor predictions on a holdout dataset.\n",
    "- *Problems with Predictions*. Problems with predictions manifest in the stochastic\n",
    "training algorithm having a strong influence on the final model, causing high variance in\n",
    "behavior and performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to Use the Framework\n",
    "\n",
    "**Step 1: Diagnose the Performance Problem**\n",
    "A robust diagnostic tool is to calculate a learning curve of loss and a\n",
    "problem-specific metric (like RMSE for regression or accuracy for classification) on a train and\n",
    "validation dataset over a given number of training epochs\n",
    "\n",
    "- If the loss on the training dataset is poor, stuck, or fails to improve, perhaps you have a\n",
    "learning problem.\n",
    "- If the loss or problem-specific metric on the training dataset continues to improve and\n",
    "gets worse on the validation dataset, perhaps you have a generalization problem.\n",
    "- If the loss or problem-specific metric on the validation dataset shows a high variance\n",
    "towards the end of the run, perhaps you have a prediction problem.\n",
    "\n",
    "**Step 2: Select and Evaluate a Technique**\n",
    "Review the techniques that are designed to address your problem. Select a technique that\n",
    "appears to be a good fit for your model and problem.\n",
    "- *Learning Problem*: Tuning the hyperparameters of the learning algorithm; specifically,\n",
    "the learning rate offers the biggest leverage.\n",
    "- *Generalization Problem*: Using weight regularization and early stopping works well\n",
    "on most models with most problems, or try dropout with early stopping.\n",
    "- *Prediction Problem*: Average the prediction from models collected over multiple runs\n",
    "or multiple epochs on one run to add sufficient bias.\n",
    "\n",
    "**Step 3: Go To Step 1**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diagnostic Learning Curves\n",
    "https://machinelearningmastery.com/learning-curves-for-diagnosing-machine-learning-model-performance/\n",
    "\n",
    "*Learning Curve: Line plot of learning (y-axis) over experience (x-axis).*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Good Fit Learning Curves\n",
    "\n",
    "A plot of learning curves shows a good fit if:\n",
    "\n",
    "- The plot of training loss decreases to a point of stability.\n",
    "- The plot of validation loss decreases to a point of stability and has a small gap with the training loss.\n",
    "\n",
    "<img height=\"400\" width=\"500\" src=\"https://machinelearningmastery.com/wp-content/uploads/2018/12/Example-of-Train-and-Validation-Learning-Curves-Showing-A-Good-Fit.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Underfit Learning Curves\n",
    "*Underfitting refers to a model that cannot learn the training dataset.*\n",
    "\n",
    "A plot of learning curves shows underfitting if:\n",
    "\n",
    "- The training loss remains flat regardless of training.\n",
    "- The training loss continues to decrease until the end of training.\n",
    "\n",
    "<img height=\"300\" width=\"400\" src=\"https://machinelearningmastery.com/wp-content/uploads/2019/02/Example-of-Training-Learning-Curve-Showing-An-Underfit-Model-That-Does-Not-Have-Sufficient-Capacity.png\">\n",
    "\n",
    "<img height=\"300\" width=\"400\" src=\"https://machinelearningmastery.com/wp-content/uploads/2018/12/Example-of-Training-Learning-Curve-Showing-An-Underfit-Model-That-Requires-Further-Training.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfit Learning Curves\n",
    "\n",
    "A plot of learning curves shows overfitting if:\n",
    "\n",
    "- The plot of training loss continues to decrease with experience.\n",
    "- The plot of validation loss decreases to a point and begins increasing again.\n",
    "\n",
    "<img height=\"500\" width=\"600\" src=\"https://machinelearningmastery.com/wp-content/uploads/2018/12/Example-of-Train-and-Validation-Learning-Curves-Showing-An-Overfit-Model.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diagnosing Unrepresentative Datasets\n",
    "\n",
    "An **unrepresentative dataset** means a dataset that may not capture the statistical characteristics relative to another dataset drawn from the same domain, such as between a train and a validation dataset. This can commonly occur if the number of samples in a dataset is too small, relative to another dataset.\n",
    "\n",
    "There are two common cases that could be observed; they are:\n",
    "- **Unrepresentative Train Dataset**:\n",
    "An unrepresentative training dataset means that the training dataset does not provide sufficient information to learn the problem, relative to the validation dataset used to evaluate it.\n",
    "\n",
    "This may occur if the training dataset has too few examples as compared to the validation dataset.\n",
    "\n",
    "This situation can be identified by a learning curve for training loss that shows improvement and similarly a learning curve for validation loss that shows improvement, but a large gap remains between both curves.\n",
    "\n",
    "<img height=\"500\" width=\"700\" src=\"https://machinelearningmastery.com/wp-content/uploads/2018/12/Example-of-Train-and-Validation-Learning-Curves-Showing-a-Training-Dataset-the-May-be-too-Small-Relative-to-the-Validation-Dataset.png\">\n",
    "\n",
    "\n",
    "- **Unrepresentative Validation Dataset**:\n",
    "An unrepresentative validation dataset means that the validation dataset does not provide sufficient information to evaluate the ability of the model to generalize.\n",
    "\n",
    "This may occur if the validation dataset has too few examples as compared to the training dataset.\n",
    "\n",
    "This case can be identified by a learning curve for training loss that looks like a good fit (or other fits) and a learning curve for validation loss that shows noisy movements around the training loss.\n",
    "\n",
    "It may also be identified by a validation loss that is lower than the training loss. In this case, it indicates that the validation dataset may be easier for the model to predict than the training dataset.\n",
    "\n",
    "<img height=\"350\" width=\"450\" src=\"https://machinelearningmastery.com/wp-content/uploads/2018/12/Example-of-Train-and-Validation-Learning-Curves-Showing-a-Validation-Dataset-the-May-be-too-Small-Relative-to-the-Training-Dataset.png\">\n",
    "<img height=\"350\" width=\"450\" src=\"https://machinelearningmastery.com/wp-content/uploads/2018/12/Example-of-Train-and-Validation-Learning-Curves-Showing-a-Validation-Dataset-that-is-Easier-to-Predict-than-the-Training-Dataset.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How To Improve Deep Learning Performance\n",
    "https://machinelearningmastery.com/improve-deep-learning-performance/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Improve Performance With Data\n",
    "\n",
    "Here’s a short list of what we’ll cover:\n",
    "\n",
    "- Get More Data: Can you get more training data?\n",
    "- Invent More Data: You can use a generative model and simple tricks with data augumentation\n",
    "- Rescale Your Data: Rescale your data to the bounds of your activation functions.\n",
    "- Transform Your Data: Guesstimate the univariate distribution of each column.\n",
    "- Feature Selection: Can you remove some attributes from your data using feature importance ?\n",
    "- Reframe Your Problem: Are the observations that you’ve collected the only way to frame your problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Improve Performance With Algorithms\n",
    "\n",
    "Here’s the short list\n",
    "\n",
    "- Spot-Check Algorithms: Spot-check a suite of top methods and see which fair well and which do not.\n",
    "- Steal From Literature: A great shortcut to picking a good method, is to steal ideas from literature.\n",
    "- Resampling Methods: Deep learning methods are slow to train. Perhaps you can perform model selection and tuning using the smaller dataset, then scale the final technique up to the full dataset at the end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Improve Performance With Algorithm Tuning\n",
    "Here are some ideas on tuning your neural network algorithms in order to get more out of them.\n",
    "\n",
    "- Diagnostics: Is your model overfitting or underfitting?\n",
    "- Weight Initialization: Initialize using small random numbers.\n",
    "- Learning Rate: Larger networks need more training, and the reverse. If you add more neurons or more layers, increase your learning rate\n",
    "- Activation Functions: Before that it was sigmoid and tanh, then a softmax, linear or sigmoid on the output layer. I don’t recommend trying more than that unless you know what you’re doing.\n",
    "- Network Topology: How many layers and how many neurons do you need? No one knows. No one. Don’t ask.\n",
    "- Batches and Epochs: The batch size defines the gradient and how often to update weights. An epoch is the entire training data exposed to the network, batch-by-batch.\n",
    "- Regularization: Regularization is a great approach to curb overfitting the training data.\n",
    "- Optimization and Loss: Have you experimented with different optimization procedures?\n",
    "- Early Stopping: You can stop learning once performance starts to degrade."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Improve Performance With Ensembles\n",
    "We’ll take a look at three general areas of ensembles you may want to consider:\n",
    "\n",
    "- Combine Models: Don’t select a model, combine them.\n",
    "- Combine Views: As above, but train each network on a different view or framing of your problem.\n",
    "- Stacking: You can also learn how to best combine the predictions from multiple models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Others Articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Training a Neural Network Is Hard\n",
    "https://machinelearningmastery.com/why-training-a-neural-network-is-hard/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    "## Recommendations for Deep Learning Neural Network Practitioners\n",
    "https://machinelearningmastery.com/recommendations-for-deep-learning-neural-network-practitioners/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8 Tricks for Configuring Backpropagation to Train Better Neural Networks\n",
    "https://machinelearningmastery.com/best-advice-for-configuring-backpropagation-for-deep-learning-neural-networks/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to Demonstrate Your Basic Skills with Deep Learning\n",
    "https://machinelearningmastery.com/how-to-demonstrate-basic-deep-learning-competence/ \n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
