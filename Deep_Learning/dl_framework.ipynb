{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. How to write deep learning Framework ? \n",
    "-  Step 1: Define Tensor class with automatic gradient computation (autograd)\n",
    "\n",
    "- Step 2: Upgrading autograd to support multiuse tensors\n",
    "\n",
    "- Step 3: Adding support for negation\n",
    "```py\n",
    "import numpy as np\n",
    "class Tensor (object):\n",
    "    def __init__(self, data, \n",
    "                creators=None, # creators is a list containing any tensors used in the creation of the current tensor\n",
    "                creation_op=None, # creation_op is a related feature that stores the instructions creators used in the creation process\n",
    "                autograd=False,\n",
    "                id=None):\n",
    "\n",
    "        self.data = np.array(data)\n",
    "        self.creation_op = creation_op\n",
    "        self.creators = creators\n",
    "        self.grad = None\n",
    "        self.autograd = autograd\n",
    "        self.children = {}\n",
    "        \"\"\"\n",
    "        You create a `self.children` counter that counts the number of gradients received \n",
    "        from each child during backpropagation. This way, you also prevent a variable from \n",
    "        accidentally backpropagating from the same child twice (which throws an exception)\n",
    "        \"\"\"\n",
    "        if id is None:\n",
    "            id = np.random.randint(0,10000)\n",
    "        self.id = id\n",
    "\n",
    "        if creators is not None:\n",
    "            for c in creators:\n",
    "                # Keeps track of how many children a tensor has\n",
    "                if self.id not in c.children:\n",
    "                    c.children[self.id] = 1\n",
    "                else:\n",
    "                    c.children[self.id] += 1\n",
    "    \"\"\"\n",
    "    Create new function `all_children_grads_accounted_for()` \n",
    "    => to compute whether a tensor has received gradients from \n",
    "    all of its children \n",
    "    \"\"\"\n",
    "    def all_children_grads_accounted_for(self):\n",
    "        for id,cnt in self.children.items():\n",
    "            \"\"\"Checks whether a tensor has received the correct \n",
    "            number of gradients from each child\"\"\"\n",
    "            if(cnt != 0):\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "    # Adding support for additional functions\n",
    "    def __add__(self, other):\n",
    "        if (self.autograd and other.autograd):\n",
    "            return Tensor(self.data + other.data,\n",
    "                            autograd=True,\n",
    "                            creators=[self,other],\n",
    "                            creation_op=\"add\")\n",
    "        return Tensor(self.data + other.data)\n",
    "\n",
    "    \n",
    "    def __mul__(self, other):\n",
    "        if(self.autograd and other.autograd):\n",
    "            return Tensor(self.data * other.data,\n",
    "                            autograd=True,\n",
    "                            creators=[self,other],\n",
    "                            creation_op=\"mul\")\n",
    "        return Tensor(self.data * other.data)\n",
    "\n",
    "    def sum(self, dim):\n",
    "        if(self.autograd):\n",
    "            return Tensor(self.data.sum(dim),\n",
    "                            autograd=True,\n",
    "                            creators=[self],\n",
    "                            creation_op=\"sum_\"+str(dim))\n",
    "        return Tensor(self.data.sum(dim))\n",
    "\n",
    "    def expand(self, dim,copies):\n",
    "        trans_cmd = list(range(0,len(self.data.shape)))\n",
    "        trans_cmd.insert(dim,len(self.data.shape))\n",
    "        new_shape = list(self.data.shape) + [copies]\n",
    "        new_data = self.data.repeat(copies).reshape(new_shape)\n",
    "        new_data = new_data.transpose(trans_cmd)\n",
    "        if(self.autograd):\n",
    "            return Tensor(new_data,\n",
    "                            autograd=True,\n",
    "                            creators=[self],\n",
    "                            creation_op=\"expand_\"+str(dim))\n",
    "        return Tensor(new_data)\n",
    "\n",
    "    def transpose(self):\n",
    "        if(self.autograd):\n",
    "            return Tensor(self.data.transpose(),\n",
    "                            autograd=True,\n",
    "                            creators=[self],\n",
    "                            creation_op=\"transpose\")\n",
    "        return Tensor(self.data.transpose())\n",
    "\n",
    "    def mm(self, x): #dot matrix\n",
    "        if(self.autograd):\n",
    "            return Tensor(self.data.dot(x.data),\n",
    "                            autograd=True,\n",
    "                            creators=[self,x],\n",
    "                            creation_op=\"mm\")\n",
    "        return Tensor(self.data.dot(x.data))\n",
    "        \n",
    "    # Adding support for negation\n",
    "    def __neg__(self):\n",
    "        if(self.autograd):\n",
    "            return Tensor(self.data * -1,\n",
    "                            autograd=True,\n",
    "                            creators=[self],\n",
    "                            creation_op=\"neg\")\n",
    "        return Tensor(self.data * -1)\n",
    "\n",
    "    def __sub__(self, other):\n",
    "        if(self.autograd and other.autograd):\n",
    "            return Tensor(self.data - other.data,\n",
    "                            autograd=True,\n",
    "                            creators=[self,other],\n",
    "                            creation_op=\"sub\")\n",
    "        return Tensor(self.data - other.data)\n",
    "    \n",
    "    def backward(self, grad=None, grad_origin=None):\n",
    "        if self.autograd:\n",
    "            if grad_origin is not None:\n",
    "                \"\"\"\n",
    "                Checks to make sure you can backpropagate or whether you're waiting for a gradient, \n",
    "                in which case decrement the counter\n",
    "                \"\"\"\n",
    "                if self.children[grad_origin.id == 0]:\n",
    "                    raise Exception(\"cannot backprop more than once\")\n",
    "                else:\n",
    "                    self.children[grad_origin.id] -= 1\n",
    "            # Accumulates gradients from several children\n",
    "            if self.grad is None:\n",
    "                self.grad = grad\n",
    "            else:\n",
    "                self.grad += grad\n",
    "            # Begin actual backpropagation\n",
    "            if (self.creators is not None and \n",
    "               (self.all_children_grads_accounted_for() or\n",
    "                grad_origin is None)):\n",
    "                if (self.creation_op == \"add\"):\n",
    "                    self.creators[0].backward(self.grad, self)\n",
    "                    self.creators[1].backward(self.grad, self)\n",
    "                if(self.creation_op == \"sub\"):\n",
    "                    new = Tensor(self.grad.data)\n",
    "                    self.creators[0].backward(new, self)\n",
    "                    new = Tensor(self.grad.__neg__().data)\n",
    "                    self.creators[1].backward(, self)\n",
    "                if(self.creation_op == \"mul\"):\n",
    "                    new = self.grad * self.creators[1]\n",
    "                    self.creators[0].backward(new , self)\n",
    "                    new = self.grad * self.creators[0]\n",
    "                    self.creators[1].backward(new, self)\n",
    "                if(self.creation_op == \"mm\"):\n",
    "                    # Usually an activation \n",
    "                    act = self.creators[0]\n",
    "                    weights = self.creators[1]\n",
    "                    new = self.grad.mm(weights.transpose())\n",
    "                    act.backward(new)\n",
    "                    new = self.grad.transpose().mm(act).transpose()\n",
    "                    weights.backward(new)\n",
    "                if(self.creation_op == \"transpose\"):\n",
    "                    self.creators[0].backward(self.grad.transpose())\n",
    "\t\t\t\t\t\t\t\n",
    "                if(\"sum\" in self.creation_op):\n",
    "                    dim = int(self.creation_op.split(\"_\")[1])\n",
    "                    ds = self.creators[0].data.shape[dim]\n",
    "                    self.creators[0].backward(self.grad.expand(dim,ds))\n",
    "                    \n",
    "                if(\"expand\" in self.creation_op):\n",
    "                    dim = int(self.creation_op.split(\"_\")[1])\n",
    "                    self.creators[0].backward(self.grad.sum(dim))\n",
    "\n",
    "    def __repr__(self):\n",
    "        return str(self.data.__repr__())\n",
    "\n",
    "    def __str__(self):\n",
    "        return str(self.data.__str__())\n",
    "```\n",
    "\n",
    "\n",
    "### Step 5: Using autograd to train a neural network\n",
    "\n",
    "### Step 6: Adding automatic optimization\n",
    "\n",
    "### Step 7: Adding support for layer types\n",
    "\n",
    "### Step 8: Layers that contain layers\n",
    "\n",
    "### Step 9: Loss-function layers"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "19985ac8a88737d3c8b7fbc1bc9ac2991a55fab1b7ef4317ae756c7f86ac40fa"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
