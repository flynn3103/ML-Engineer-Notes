{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is a deep learning framework?\n",
    "- Good tools reduce errors, speed development, and increase\n",
    "runtime performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduce Tensor\n",
    "*Tensors are an abstract form of vectors and matrices.*\n",
    "\n",
    "Up to this point, we’ve been working exclusively with vectors and matrices as the basic data\n",
    "structures for deep learning. Recall that a matrix is a list of vectors, and a vector is a list\n",
    "of scalars (single numbers). A tensor is the abstract version of this form of nested lists of\n",
    "numbers. A vector is a one-dimensional tensor. A matrix is a two-dimensional tensor, and\n",
    "higher dimensions are referred to as n-dimensional tensors. Thus, the beginning of a new\n",
    "deep learning framework is the construction of this basic type, which we’ll call `Tensor` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Tensor (object):\n",
    "    def __init__(self, data):\n",
    "        self.data = np.array(data)\n",
    "    def __add__(self, other):\n",
    "        return Tensor(self.data + other.data)\n",
    "    def __repr__(self):\n",
    "        return str(self.data.__repr__())\n",
    "    def __str__(self):\n",
    "        return str(self.data.__str__())\n",
    "\n",
    "x = Tensor([1,2,3,4,5])\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to automatic gradient computation (autograd)\n",
    "*Previously, you performed backpropagation by hand.*\n",
    "\n",
    "Recall that this is done by moving backward\n",
    "through the neural network: first compute the gradient at the output of the network, then\n",
    "use that result to compute the derivative at the next-to-last component, and so on until all\n",
    "weights in the architecture have correct gradients. This logic for computing gradients can\n",
    "also be added to the tensor object. Let me show you what I mean.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tensor (object):\n",
    "    def __init__(self, data, creators=None, creation_op=None):\n",
    "        self.data = np.array(data)\n",
    "        self.creation_op = creation_op\n",
    "        self.creators = creators\n",
    "        self.grad = None\n",
    "\n",
    "    def backward(self, grad):\n",
    "        self.grad = grad\n",
    "        if(self.creation_op == \"add\"):\n",
    "            self.creators[0].backward(grad)\n",
    "            self.creators[1].backward(grad)\n",
    "    \n",
    "    def __add__(self, other):\n",
    "        return Tensor(self.data + other.data,\n",
    "                     creators=[self, other],\n",
    "                     creation_op=\"add\")\n",
    "\n",
    "    def __repr__(self):\n",
    "        return str(self.data.__repr__())\n",
    "        \n",
    "    def __str__(self):\n",
    "        return str(self.data.__str__())\n",
    "\n",
    "x = Tensor([1,2,3,4,5])\n",
    "y = Tensor([2,2,2,2,2])\n",
    "\n",
    "z = x + y\n",
    "z.backward(Tensor(np.array([1,1,1,1,1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method introduces two new concepts:\n",
    "- First, each tensor gets two new attributes.\n",
    "creators is a list containing any tensors used in the creation of the current tensor (which\n",
    "defaults to None ). \n",
    "- Thus, when the two tensors x and y are added together, z has two creators , x and y . creation_op is a related feature that stores the instructions creators\n",
    "used in the creation process. \n",
    "- Thus, performing z = x + y creates a computation graph with\n",
    "three nodes ( x , y , and z ) and two edges ( z -> x and z -> y ). Each edge is labeled by the\n",
    "creation_op add . This graph allows you to recursively backpropagate gradients.\n",
    "\n",
    "The second new concept introduced in this version of Tensor is the ability to use this graph\n",
    "to compute gradients. When you call z .backward() , it sends the correct gradient for x\n",
    "and y given the function that was applied to create z ( add )."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A quick checkpoint\n",
    "*Everything in Tensor is another form of lessons already learned.*\n",
    "\n",
    "You had a list of layers (dictionaries) and hand-coded the correct order of forward and\n",
    "backpropagation operations. Now you’re building a nice interface so you don’t have to\n",
    "write as much code.\n",
    "\n",
    "- In particular, this notion of a graph that gets\n",
    "built during forward propagation is called a *dynamic computation graph* because it’s built\n",
    "on the fly during forward prop. \n",
    "- In general, dynamic computation graphs are easier to write/experiment with, and static\n",
    "computation graphs have faster runtimes because of some fancy logic under the hood.\n",
    "- But note that dynamic and static frameworks have lately been moving toward the middle:\n",
    "    - allowing dynamic graphs to compile to static ones (for faster runtimes) or\n",
    "    - allowing static graphs to be built dynamically (for easier experimentation)\n",
    "- The primary difference is whether forward propagation is happening during graph construction or after the graph is already defined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensors that are used multiple times\n",
    "*The basic autograd has a rather pesky bug. Let’s squish it!*\n",
    "\n",
    "The current version of Tensor supports backpropagating into a variable only once. But\n",
    "sometimes, during forward propagation, you’ll use the same tensor multiple times (the\n",
    "weights of a neural network), and thus multiple parts of the graph will backpropagate\n",
    "gradients into the same tensor.\n",
    "\n",
    "<img src=\"../../images/tensor_multiuse.png\">\n",
    "\n",
    "The code will currently compute the incorrect gradient when backpropagating into a variable that was used multiple times (is the parent of multiple\n",
    "children).\n",
    "\n",
    "The current implementation of Tensor merely overwrites each derivative with the\n",
    "previous. First, d applies its gradient, and then it gets overwritten with the gradient from e .\n",
    "We need to change the way gradients are written."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upgrading autograd to support multiuse tensors\n",
    "*Add one new function, and update three old ones.*\n",
    "\n",
    "This update to the Tensor object adds two new features. First, gradients can be accumulated so\n",
    "that when a variable is used more than once, it receives gradients from all children\n",
    "\n",
    "- Create a self.children counter that counts the number of gradients\n",
    "received from each child during backpropagation. This way, you also prevent a variable from\n",
    "accidentally backpropagating from the same child twice (which throws an exception).\n",
    "- The second added feature is a new function with the rather verbose name `all_children_\n",
    "grads_accounted_for()`. \n",
    "- The purpose of this function is to compute whether a tensor hasreceived gradients from all of its children in the graph. Normally, whenever .backward() is called on an intermediate variable in a graph, it immediately calls .backward() on its parents.\n",
    "- But because some variables receive their gradient value from multiple parents, each variable\n",
    "needs to wait to call .backward() on its parents until it has the final gradient locally.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding support for negation\n",
    "*Let’s modify the support for addition to support negation.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding support for additional functions\n",
    "*Subtraction, multiplication, sum, expand, transpose, and matrix multiplication*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tensor (object):\n",
    "    def __init__(self, data, \n",
    "                autograd=False,\n",
    "                creators=None, \n",
    "                creation_op=None,\n",
    "                id=None):\n",
    "        self.data = np.array(data)\n",
    "        self.creation_op = creation_op\n",
    "        self.creators = creators\n",
    "        self.grad = None\n",
    "        self.autograd = autograd\n",
    "        self.children = {}\n",
    "        if(id is None):\n",
    "            id = np.random.randint(0,1000000)\n",
    "        self.id = id\n",
    "\n",
    "        if(creators is not None):\n",
    "            for c in creators:\n",
    "                # Keeps track of how many children a tensor has\n",
    "                if(self.id not in c.children):\n",
    "                    c.children[self.id] = 1\n",
    "                else:\n",
    "                    c.children[self.id] += 1\n",
    "                    \n",
    "    #Check whether a tensor has received the correct num of gradients from each child\n",
    "    def all_children_grads_accounted_for(self):\n",
    "        for id, cnt in self.children.items():\n",
    "            if(cnt != 0):\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "    def backward(self, grad=None, grad_origin=None):\n",
    "        if(self.autograd):\n",
    "            if(grad_origin is not None):\n",
    "                #  Make sure u can backprop or if u're waiting for a gradient\n",
    "                # in which case decrement the counter\n",
    "                if(self.children[grad_origin.id] == 0):\n",
    "                    raise Exception(\"cannot backprop more than once\")\n",
    "                else:\n",
    "                    self.children[grad_origin.id] -= 1\n",
    "            # Accumulates gradients from several children        \n",
    "            if(self.grad is None):\n",
    "                self.grad = grad\n",
    "            else:\n",
    "                self.grad += grad\n",
    "            \n",
    "            if (self.creators is not None and \n",
    "                (self.all_children_grads_accounted_for() or\n",
    "                grad_origin is None)):\n",
    "                if(self.creation_op == \"add\"):\n",
    "                    self.creators[0].backward(self.grad, self)\n",
    "                    self.creators[1].backward(self.grad, self)\n",
    "                if(self.creation_op == \"neg\"):\n",
    "                    self.creators[0].backward(self.grad.__neg__())\n",
    "                if(self.creation_op == \"sub\"):\n",
    "                    new = Tensor(self.grad.data)\n",
    "                    self.creators[0].backward(new, self)\n",
    "                    new = Tensor(self.grad.__neg__().data)\n",
    "                    self.creators[1].backward(self)\n",
    "                if(self.creation_op == \"mul\"):\n",
    "                    new = self.grad * self.creators[1]\n",
    "                    self.creators[0].backward(new , self)\n",
    "                    new = self.grad * self.creators[0]\n",
    "                    self.creators[1].backward(new, self)\n",
    "                if(self.creation_op == \"mm\"):\n",
    "                    #Usually an activation\n",
    "                    act = self.creators[0] \n",
    "                    # weight matrix\n",
    "                    weights = self.creators[1]\n",
    "                    new = self.grad.mm(weights.transpose())\n",
    "                    act.backward(new)\n",
    "                    new = self.grad.transpose().mm(act).transpose()\n",
    "                    weights.backward(new)\n",
    "                if(self.creation_op == \"transpose\"):\n",
    "                    self.creators[0].backward(self.grad.transpose())\n",
    "                if(\"sum\" in self.creation_op):\n",
    "                    dim = int(self.creation_op.split(\"_\")[1])\n",
    "                    ds = self.creators[0].data.shape[dim]\n",
    "                    self.creators[0].backward(self.grad.expand(dim,ds))\n",
    "                if(\"expand\" in self.creation_op):\n",
    "                    dim = int(self.creation_op.split(\"_\")[1])\n",
    "                    self.creators[0].backward(self.grad.sum(dim))\n",
    "\n",
    "    def __add__(self, other):\n",
    "        if(self.autograd and other.autograd):\n",
    "            return Tensor(self.data + other.data,\n",
    "                        autograd=True,\n",
    "                        creators=[self, other],\n",
    "                        creation_op=\"add\")\n",
    "        return Tensor(self.data + other.data)\n",
    "\n",
    "    def __neg__(self):\n",
    "        if(self.autograd):\n",
    "            return Tensor(self.data * -1,\n",
    "                        autograd=True,\n",
    "                        creators=[self],\n",
    "                        creation_op=\"neg\")\n",
    "        return Tensor(self.data * -1)\n",
    "\n",
    "    def __sub__(self, other):\n",
    "        if(self.autograd and other.autograd):\n",
    "            return Tensor(self.data - other.data,\n",
    "                            autograd=True,\n",
    "                            creators=[self,other],\n",
    "                            creation_op=\"sub\")\n",
    "        \n",
    "        return Tensor(self.data - other.data)\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        if(self.autograd and other.autograd):\n",
    "            return Tensor(self.data * other.data,\n",
    "                            autograd=True,\n",
    "                            creators=[self,other],\n",
    "                            creation_op=\"mul\")\n",
    "        return Tensor(self.data * other.data)\n",
    "\n",
    "    def sum(self, dim):\n",
    "        if(self.autograd):\n",
    "            return Tensor(self.data.sum(dim),\n",
    "                            autograd=True,\n",
    "                            creators=[self],\n",
    "                            creation_op=\"sum_\"+str(dim))\n",
    "        return Tensor(self.data.sum(dim))\n",
    "    \n",
    "    def expand(self, dim,copies):\n",
    "        trans_cmd = list(range(0,len(self.data.shape)))\n",
    "        trans_cmd.insert(dim,len(self.data.shape))\n",
    "        new_shape = list(self.data.shape) + [copies]\n",
    "        new_data = self.data.repeat(copies).reshape(new_shape)\n",
    "        new_data = new_data.transpose(trans_cmd)\n",
    "        if(self.autograd):\n",
    "            return Tensor(new_data,\n",
    "                            autograd=True,\n",
    "                            creators=[self],\n",
    "                            creation_op=\"expand_\"+str(dim))\n",
    "        return Tensor(new_data)\n",
    "\n",
    "\n",
    "    def transpose(self):\n",
    "        if(self.autograd):\n",
    "            return Tensor(self.data.transpose(),\n",
    "                            autograd=True,\n",
    "                            creators=[self],\n",
    "                            creation_op=\"transpose\")\n",
    "        return Tensor(self.data.transpose())\n",
    "\n",
    "    def mm(self, x):\n",
    "        if(self.autograd):\n",
    "            return Tensor(self.data.dot(x.data),\n",
    "                            autograd=True,\n",
    "                            creators=[self,x],\n",
    "                            creation_op=\"mm\")\n",
    "        return Tensor(self.data.dot(x.data))\n",
    "\n",
    "\n",
    "    def __repr__(self):\n",
    "        return str(self.data.__repr__())\n",
    "        \n",
    "    def __str__(self):\n",
    "        return str(self.data.__str__())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Tensor(np.array([[1,2,3], [4,5,6]]))\n",
    "x.sum(0)\n",
    "\n",
    "x.expand(dim=0, copies=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using autograd to train a neural network\n",
    "*You no longer have to write backpropagation logic!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "np.random.seed(0)\n",
    "data = Tensor(np.array([[0,0],[0,1],[1,0],[1,1]]), autograd=True)\n",
    "target = Tensor(np.array([[0],[1],[0],[1]]), autograd=True)\n",
    "\n",
    "w = list()\n",
    "w.append(Tensor(np.random.rand(2,3), autograd=True))\n",
    "w.append(Tensor(np.random.rand(3,1), autograd=True))\n",
    "\n",
    "for i in range(10):\n",
    "    pred = data.mm(w[0]).mm(w[1])\n",
    "\n",
    "    loss = ((pred - target)*(pred - target)).sum(0)\n",
    "    loss.backward(Tensor(np.ones_like(loss.data)))\n",
    "\n",
    "    for w_ in w:\n",
    "        w_.data -= w_.grad.data * 0.1\n",
    "        w_.grad.data *= 0\n",
    "        print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding automatic optimization\n",
    "*Let’s make a stochastic gradient descent optimizer.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD(object):\n",
    "    def __init__(self, parameters, alpha=0.1):\n",
    "        self.parameters = parameters\n",
    "        self.alpha = alpha\n",
    "    def zero(self):\n",
    "        for p in self.parameters:\n",
    "            p.grad.data *= 0\n",
    "    def step(self, zero=True):\n",
    "        for p in self.parameters:\n",
    "            p.data -= p.grad.data * self.alpha\n",
    "            if(zero):\n",
    "                p.grad.data *= 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "data = Tensor(np.array([[0,0],[0,1],[1,0],[1,1]]), autograd=True)\n",
    "target = Tensor(np.array([[0],[1],[0],[1]]), autograd=True)\n",
    "\n",
    "w = list()\n",
    "w.append(Tensor(np.random.rand(2,3), autograd=True))\n",
    "w.append(Tensor(np.random.rand(3,1), autograd=True))\n",
    "\n",
    "optim = SGD(parameters=w, alpha=0.1)\n",
    "\n",
    "for i in range(10):\n",
    "    pred = data.mm(w[0]).mm(w[1])\n",
    "    loss = ((pred - target)*(pred - target)).sum(0)\n",
    "    loss.backward(Tensor(np.ones_like(loss.data)))\n",
    "    optim.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding support for layer types\n",
    "*You may be familiar with layer types in Keras or PyTorch.*\n",
    "\n",
    "- The weights are organized into a class (and I added bias\n",
    "weights because this is a true linear layer). You can initialize the layer all together, such\n",
    "that both the weights and bias are initialized with the correct sizes, and the correct forward\n",
    "propagation logic is always employed.\n",
    "\n",
    "- Also notice that I created an abstract class Layer , which has a single getter. This allows for\n",
    "more-complicated layer types (such as layers containing other layers). All you need to do is\n",
    "override get_parameters() to control what tensors are later passed to the optimizer (such\n",
    "as the SGD class created in the previous section)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer(object):\n",
    "    def __init__(self):\n",
    "        self.parameters = list()\n",
    "    def get_parameters(self):\n",
    "        return self.parameters\n",
    "    \n",
    "class Linear(Layer):\n",
    "    def __init__(self, n_inputs, n_outputs):\n",
    "        super().__init__()\n",
    "        W = np.random.randn(n_inputs, n_outputs) * np.sqrt(2.0/(n_inputs))\n",
    "        self.weight = Tensor(W, autograd=True)\n",
    "        self.bias = Tensor(np.zeros(n_outputs), autograd=True)\n",
    "\n",
    "        self.parameters.append(self.weight)\n",
    "        self.parameters.append(self.bias)\n",
    "\n",
    "    def forward(self, input):\n",
    "        return input.mm(self.weight) + self.bias.expand(0, len(input.data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layers that contain layers\n",
    "*Layers can also contain other layers*\n",
    "\n",
    "The most popular layer is a sequential layer that forward propagates a list of layers, where\n",
    "each layer feeds its outputs into the inputs of the next layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequential(Layer):\n",
    "    def __init__(self, layers=list()):\n",
    "        super().__init__()\n",
    "        self.layers = layers\n",
    "\n",
    "    def add(self, layer):\n",
    "        self.layers.append(layer)\n",
    "\n",
    "    def forward(self, input):\n",
    "        for layer in self.layers:\n",
    "            input = layer.forward(input)\n",
    "        return input\n",
    "\n",
    "    def get_parameters(self):\n",
    "        params = list()\n",
    "        for l in self.layers:\n",
    "            params += l.get_parameters()\n",
    "        return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Tensor(np.array([[0,0],[0,1],[1,0],[1,1]]), autograd=True)\n",
    "target = Tensor(np.array([[0],[1],[0],[1]]), autograd=True)\n",
    "model = Sequential([Linear(2,3), Linear(3,1)])\n",
    "optim = SGD(parameters=model.get_parameters(), alpha=0.05)\n",
    "for i in range(10):\n",
    "    pred = model.forward(data)\n",
    "    loss = ((pred - target)*(pred - target)).sum(0)\n",
    "    loss.backward(Tensor(np.ones_like(loss.data)))\n",
    "    optim.step()\n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss-function layers\n",
    "You can also create layers that are functions on the input. The most popular version of this\n",
    "kind of layer is probably the loss-function layer, such as mean squared error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSELoss(Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        return ((pred - target)*(pred - target)).sum(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Tensor(np.array([[0,0],[0,1],[1,0],[1,1]]), autograd=True)\n",
    "target = Tensor(np.array([[0],[1],[0],[1]]), autograd=True)\n",
    "model = Sequential([Linear(2,3), Linear(3,1)])\n",
    "optim = SGD(parameters=model.get_parameters(), alpha=0.05)\n",
    "\n",
    "criterion = MSELoss()\n",
    "\n",
    "for i in range(10):\n",
    "    pred = model.forward(data)\n",
    "    loss = criterion.forward(pred, target)\n",
    "    loss.backward(Tensor(np.ones_like(loss.data)))\n",
    "    optim.step()\n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nonlinearity layers\n",
    "\n",
    "Let’s add nonlinear functions to Tensor and then create some\n",
    "layer types."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s add them to the Tensor\n",
    "class. You learned about the derivative for both quite some time ago, so this should be easy:\n",
    "\n",
    "```py\n",
    "def sigmoid(self):\n",
    "    if(self.autograd):\n",
    "        return Tensor(1 / (1 + np.exp(-self.data)),\n",
    "                    autograd=True,\n",
    "                    creators=[self],\n",
    "                    creation_op='sigmoid')\n",
    "def tanh(self):\n",
    "    if(self.autograd):\n",
    "        return Tensor(np.tanh(-self.data),\n",
    "                    autograd=True,\n",
    "                    creators=[self],\n",
    "                    creation_op='tanh')\n",
    "    return Tensor(np.tanh(self.data))\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code shows the backprop logic added to the Tensor.backward() method:\n",
    "```py\n",
    "if(self.creation_op == \"sigmoid\"):\n",
    "    ones = Tensor(np.ones_like(self.grad.data))\n",
    "    self.creators[0].backward(self.grad * (self * (ones - self)))\n",
    "\n",
    "if(self.creation_op == \"tanh\"):\n",
    "    ones = Tensor(np.ones_like(self.grad.data))\n",
    "    self.creators[0].backward(self.grad * (ones - (self * self)))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tanh(Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    def forward(self, input):\n",
    "        return input.tanh()\n",
    "\n",
    "class Sigmoid(Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    def forward(self, input):\n",
    "        return input.sigmoid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([Linear(2,3), Tanh(), Linear(3,1), Sigmoid()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding Layer\n",
    "*An embedding layer translates indices into activations.*\n",
    "\n",
    "So far, so good. The matrix has a row (vector) for each word in the vocabulary. Now, how\n",
    "will you forward propagate? Well, forward propagation always starts with the question,\n",
    "“How will the inputs be encoded?” In the case of word embeddings, you obviously can’t pass\n",
    "in the words themselves, because the words don’t tell you which rows in self.weight to\n",
    "forward propagate with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding indexing to autograd\n",
    "*Before you can build the embedding layer, autograd needs to\n",
    "support indexing.*\n",
    "\n",
    "In order to support the new embedding strategy (which assumes words are forward\n",
    "propagated as matrices of indices), the indexing you played around with in the previous\n",
    "section must be supported by autograd. This is a pretty simple idea:\n",
    "You need to make sure that during backpropagation, the gradients are placed in the same rows as were indexed into for forward propagation.\n",
    "\n",
    "```py\n",
    "def index_select(self, indices):\n",
    "    if(self.autograd):\n",
    "        new = Tensor(self.data[indices.data],\n",
    "                    autograd=True,\n",
    "                    creators=[self],\n",
    "                    creation_op=\"index_select\")\n",
    "        new.index_select_indices = indices\n",
    "        return new\n",
    "    return Tensor(self.data[indices.data])\n",
    "```\n",
    "\n",
    "```py\n",
    "if(self.creation_op == \"index_select\"):\n",
    "    new_grad = np.zeros_like(self.creators[0].data)\n",
    "    indices_ = self.index_select_indices.data.flatten()\n",
    "    grad_ = grad.data.reshape(len(indices_), -1)\n",
    "    for i in range(len(indices_)):\n",
    "        new_grad[indices_[i]] += grad_[i]\n",
    "    self.creators[0].backward(Tensor(new_grad))\n",
    "```\n",
    "- First, use the NumPy trick to select the correct rows \n",
    "- Then, during backprop() , initialize a new gradient of the correct size (the size of the\n",
    "original matrix that was being indexed into)\n",
    "- Second, flatten the indices so you can iterate\n",
    "through them\n",
    "- Third, collapse grad_ to a simple list of rows. (The subtle part is that the list\n",
    "of indices in indices_ and the list of vectors in grad_ will be in the corresponding order.)\n",
    "- Then, iterate through each index, add it into the correct row of the new gradient you’re\n",
    "creating, and backpropagate it into self.creators[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(Layer):\n",
    "    def __init__(self, vocab_size, dim):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.dim = dim\n",
    "        \n",
    "        weight = (np.random.rand(vocab_size, dim) - 0.5) / dim\n",
    "        self.weight = Tensor(weight, autograd=True)\n",
    "        self.parameters.append(self.weight)\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.weight.index_select(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = Embedding(5,3)\n",
    "model = Sequential([embed, Tanh(), Linear(3,1), Sigmoid()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The cross-entropy layer\n",
    "*Let’s add cross entropy to the autograd and create a layer.*\n",
    "\n",
    "```py\n",
    "def cross_entropy(self, target_indices):\n",
    "    temp = np.exp(self.data)\n",
    "    softmax_output = temp / np.sum(temp,\n",
    "                                    axis=len(self.data.shape)-1,\n",
    "                                    keepdims=True)\n",
    "    t = target_indices.data.flatten()\n",
    "    p = softmax_output.reshape(len(t),-1)\n",
    "    target_dist = np.eye(p.shape[1])[t]\n",
    "    loss = -(np.log(p) * (target_dist)).sum(1).mean()\n",
    "    if(self.autograd):\n",
    "        out = Tensor(loss,\n",
    "                    autograd=True,\n",
    "                    creators=[self],\n",
    "                    creation_op=\"cross_entropy\")\n",
    "        out.softmax_output = softmax_output\n",
    "        out.target_dist = target_dist\n",
    "        return out\n",
    "    return Tensor(loss)\n",
    "```\n",
    "\n",
    "One noticeable thing about this loss is different from others:\n",
    "both the final softmax and the computation of the loss are within the loss class. This is an\n",
    "extremely common convention in deep neural networks. Nearly every framework will work\n",
    "this way. \n",
    "\n",
    "When you want to finish a network and train with cross entropy, you can leave\n",
    "off the softmax from the forward propagation step and call a cross-entropy class that will\n",
    "automatically perform the softmax as a part of the loss function.\n",
    "\n",
    "The reason these are combined so consistently is performance. It’s much faster to calculate\n",
    "the gradient of softmax and negative log likelihood together in a cross-entropy function\n",
    "than to forward propagate and backpropagate them separately in two different modules.\n",
    "This has to do with a shortcut in the gradient math."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEntropyLoss(object):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    def forward(self, input, target):\n",
    "        return input.cross_entropy(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = CrossEntropyLoss()\n",
    "loss = criterion.forward(pred, target)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The recurrent neural network layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNCell(Layer):\n",
    "    def __init__(self, n_inputs,n_hidden,n_output,activation='sigmoid'):\n",
    "        super().__init__()\n",
    "        self.n_inputs = n_inputs\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_output = n_output\n",
    "        if(activation == 'sigmoid'):\n",
    "            self.activation = Sigmoid()\n",
    "        elif(activation == 'tanh'):\n",
    "            self.activation == Tanh()\n",
    "        else:\n",
    "            raise Exception(\"Non-linearity not found\")\n",
    "        self.w_ih = Linear(n_inputs, n_hidden)\n",
    "        self.w_hh = Linear(n_hidden, n_hidden)\n",
    "        self.w_ho = Linear(n_hidden, n_output)\n",
    "        \n",
    "        self.parameters += self.w_ih.get_parameters()\n",
    "        self.parameters += self.w_hh.get_parameters()\n",
    "        self.parameters += self.w_ho.get_parameters()\n",
    "    def forward(self, input, hidden):\n",
    "        from_prev_hidden = self.w_hh.forward(hidden)\n",
    "        combined = self.w_ih.forward(input) + from_prev_hidden\n",
    "        new_hidden = self.activation.forward(combined)\n",
    "        output = self.w_ho.forward(new_hidden)\n",
    "        return output, new_hidden\n",
    "    def init_hidden(self, batch_size=1):\n",
    "        return Tensor(np.zeros((batch_size,self.n_hidden)),autograd=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys,random,math\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "f = open('tasksv11/en/qa1_single-supporting-fact_train.txt','r')\n",
    "raw = f.readlines()\n",
    "f.close()\n",
    "tokens = list()\n",
    "for line in raw[0:1000]:\n",
    "    tokens.append(line.lower().replace(\"\\n\",\"\").split(\" \")[1:])\n",
    "new_tokens = list()\n",
    "for line in tokens:\n",
    "    new_tokens.append(['-'] * (6 - len(line)) + line)\n",
    "tokens = new_tokens\n",
    "\n",
    "vocab = set()\n",
    "for sent in tokens:\n",
    "    for word in sent:\n",
    "        vocab.add(word)\n",
    "vocab = list(vocab)\n",
    "\n",
    "word2index = {}\n",
    "for i,word in enumerate(vocab):\n",
    "    word2index[word]=i\n",
    "def words2indices(sentence):\n",
    "    idx = list()\n",
    "    for word in sentence:\n",
    "        idx.append(word2index[word])\n",
    "    return idx\n",
    "indices = list()\n",
    "for line in tokens:\n",
    "    idx = list()\n",
    "    for w in line:\n",
    "        idx.append(word2index[w])\n",
    "    indices.append(idx)\n",
    "data = np.array(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = Embedding(vocab_size=len(vocab),dim=16)\n",
    "model = RNNCell(n_inputs=16, n_hidden=16, n_output=len(vocab))\n",
    "criterion = CrossEntropyLoss()\n",
    "params = model.get_parameters() + embed.get_parameters()\n",
    "optim = SGD(parameters=params, alpha=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for iter in range(1000):\n",
    "    batch_size = 100\n",
    "    total_loss = 0\n",
    "    hidden = model.init_hidden(batch_size=batch_size)\n",
    "    for t in range(5):\n",
    "        input = Tensor(data[0:batch_size,t], autograd=True)\n",
    "        rnn_input = embed.forward(input=input)\n",
    "        output, hidden = model.forward(input=rnn_input, hidden=hidden)\n",
    "    target = Tensor(data[0:batch_size,t+1], autograd=True)\n",
    "    loss = criterion.forward(output, target)\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    total_loss += loss.data\n",
    "    if(iter % 200 == 0):\n",
    "        p_correct = (target.data == np.argmax(output.data,axis=1)).mean()\n",
    "        print_loss = total_loss / (len(data)/batch_size)\n",
    "        print(\"Loss:\",print_loss,\"% Correct:\",p_correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "hidden = model.init_hidden(batch_size=batch_size)\n",
    "for t in range(5):\n",
    "    input = Tensor(data[0:batch_size,t], autograd=True)\n",
    "    rnn_input = embed.forward(input=input)\n",
    "    output, hidden = model.forward(input=rnn_input, hidden=hidden)\n",
    "target = Tensor(data[0:batch_size,t+1], autograd=True)\n",
    "loss = criterion.forward(output, target)\n",
    "ctx = \"\"\n",
    "for idx in data[0:batch_size][0][0:-1]:\n",
    "    ctx += vocab[idx] + \" \"\n",
    "print(\"Context:\",ctx)\n",
    "print(\"Pred:\", vocab[output.data.argmax()])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "19985ac8a88737d3c8b7fbc1bc9ac2991a55fab1b7ef4317ae756c7f86ac40fa"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
