{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Data Cleaning\n",
    "link: https://machinelearningmastery.com/basic-data-cleaning-for-machine-learning/\n",
    "\n",
    "This tutorial is divided into seven parts; they are:\n",
    "1. Messy Datasets\n",
    "2. Identify Columns That Contain a Single Value\n",
    "3. Delete Columns That Contain a Single Value\n",
    "4. Consider Columns That Have Very Few Values\n",
    "5. Remove Columns That Have A Low Variance\n",
    "6. Identify Rows that Contain Duplicate Data\n",
    "7. Delete Rows that Contain Duplicate Data\n",
    "\n",
    "\n",
    "Data cleaning refers to identifying and correcting errors in the dataset that may negatively impact a predictive model. There are many types of errors that exist in a dataset, although some of the simplest errors\n",
    "include columns that don’t contain much information and duplicated rows. Before we dive into identifying and correcting messy data, let’s define some messy datasets\n",
    "\n",
    "## Identify Columns That Contain a Single Value\n",
    "\n",
    "Columns that have a single observation or value are probably useless for modeling. These columns or predictors are referred to zero-variance predictors as if we measured the variance\n",
    "(average value from the mean), it would be zero.\n",
    "\n",
    "Columns that have a single value for all rows do not contain any information for modeling.\n",
    "Depending on the choice of data preparation and modeling algorithms, variables with a single value can also cause errors or unexpected results. You can detect rows that have this property using the `unique()` NumPy function that will report the number of unique values in each\n",
    "column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0, 238, 25.4% \n",
      " 1, 297, 31.7% \n",
      " 2, 927, 98.9% \n",
      " 3, 933, 99.6% \n",
      " 4, 179, 19.1% \n",
      " 5, 375, 40.0% \n",
      " 6, 820, 87.5% \n",
      " 7, 618, 66.0% \n",
      " 8, 561, 59.9% \n",
      " 9, 57, 6.1% \n",
      " 10, 577, 61.6% \n",
      " 11, 59, 6.3% \n",
      " 12, 73, 7.8% \n",
      " 13, 107, 11.4% \n",
      " 14, 53, 5.7% \n",
      " 15, 91, 9.7% \n",
      " 16, 893, 95.3% \n",
      " 17, 810, 86.4% \n",
      " 18, 170, 18.1% \n",
      " 19, 53, 5.7% \n",
      " 20, 68, 7.3% \n",
      " 21, 9, 1.0% \n",
      " 22, 1, 0.1% \n",
      " 23, 92, 9.8% \n",
      " 24, 9, 1.0% \n",
      " 25, 8, 0.9% \n",
      " 26, 9, 1.0% \n",
      " 27, 308, 32.9% \n",
      " 28, 447, 47.7% \n",
      " 29, 392, 41.8% \n",
      " 30, 107, 11.4% \n",
      " 31, 42, 4.5% \n",
      " 32, 4, 0.4% \n",
      " 33, 45, 4.8% \n",
      " 34, 141, 15.0% \n",
      " 35, 110, 11.7% \n",
      " 36, 3, 0.3% \n",
      " 37, 758, 80.9% \n",
      " 38, 9, 1.0% \n",
      " 39, 9, 1.0% \n",
      " 40, 388, 41.4% \n",
      " 41, 220, 23.5% \n",
      " 42, 644, 68.7% \n",
      " 43, 649, 69.3% \n",
      " 44, 499, 53.3% \n",
      " 45, 2, 0.2% \n",
      " 46, 937, 100.0% \n",
      " 47, 169, 18.0% \n",
      " 48, 286, 30.5% \n",
      " 49, 2, 0.2% \n"
     ]
    }
   ],
   "source": [
    "# summarize the percentage of unique values for each column using numpy\n",
    "from numpy import loadtxt\n",
    "from numpy import unique\n",
    "# load the dataset\n",
    "data = loadtxt( '../../Datasets/oil-spill.csv' , delimiter= ',' )\n",
    "# summarize the number of unique values in each column\n",
    "for i in range(data.shape[1]):\n",
    "  num = len(unique(data[:, i]))\n",
    "  percentage = float(num) / data.shape[0] * 100\n",
    "  print( ' %d, %d, %.1f%% ' % (i, num, percentage))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can update the example to only summarize those variables that have unique values that\n",
    "are less than 1 percent of the number of rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 21, 9, 1.0% \n",
      " 22, 1, 0.1% \n",
      " 24, 9, 1.0% \n",
      " 25, 8, 0.9% \n",
      " 26, 9, 1.0% \n",
      " 32, 4, 0.4% \n",
      " 36, 3, 0.3% \n",
      " 38, 9, 1.0% \n",
      " 39, 9, 1.0% \n",
      " 45, 2, 0.2% \n",
      " 49, 2, 0.2% \n"
     ]
    }
   ],
   "source": [
    "# summarize the number of unique values in each column\n",
    "for i in range(data.shape[1]):\n",
    "    num = len(unique(data[:, i]))\n",
    "    percentage = float(num) / data.shape[0] * 100\n",
    "    if percentage < 1:\n",
    "        print( ' %d, %d, %.1f%% ' % (i, num, percentage))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(937, 50)\n",
      "[21, 22, 24, 25, 26, 32, 36, 38, 39, 45, 49]\n",
      "(937, 39)\n"
     ]
    }
   ],
   "source": [
    "# delete columns where number of unique values is less than 1% of the rows\n",
    "from pandas import read_csv\n",
    "# load the dataset\n",
    "df = read_csv( '../../Datasets/oil-spill.csv' , header=None)\n",
    "print(df.shape)\n",
    "# get number of unique values for each column\n",
    "counts = df.nunique()\n",
    "# record columns to delete\n",
    "to_del = [i for i,v in enumerate(counts) if (float(v)/df.shape[0]*100) < 1]\n",
    "print(to_del)\n",
    "# drop useless columns\n",
    "df.drop(to_del, axis=1, inplace=True)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Columns That Have A Low Variance\n",
    "Another approach to the problem of removing columns with few unique values is to consider\n",
    "the variance of the column. Recall that the variance is a statistic calculated on a variable as the\n",
    "average squared difference of values in the sample from the mean. The variance can be used as a\n",
    "filter for identifying columns to be removed from the dataset. A column that has a single value\n",
    "has a variance of 0.0, and a column that has very few unique values may have a small variance.\n",
    "\n",
    "The `VarianceThreshold` class from the scikit-learn library supports this as a type of feature\n",
    "selection. An instance of the class can be created and we can specify the threshold argument,\n",
    "which defaults to 0.0 to remove columns with a single value. It can then be fit and applied\n",
    "to a dataset by calling the fit transform() function to create a transformed version of the\n",
    "dataset where the columns that have a variance lower than the threshold have been removed\n",
    "automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(937, 38) (937,)\n",
      "(937, 38)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "# split data into inputs and outputs\n",
    "data = df.values\n",
    "X = data[:, :-1]\n",
    "y = data[:, -1]\n",
    "print(X.shape, y.shape)\n",
    "# define the transform\n",
    "transform = VarianceThreshold()\n",
    "# transform the input data\n",
    "X_sel = transform.fit_transform(X)\n",
    "print(X_sel.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import arange\n",
    "from matplotlib import pyplot\n",
    "# define thresholds to check\n",
    "thresholds = arange(0.0, 0.55, 0.05)\n",
    "# apply transform with each threshold\n",
    "results = list()\n",
    "for t in thresholds:\n",
    "    # define the transform\n",
    "    transform = VarianceThreshold(threshold=t)\n",
    "    # transform the input data\n",
    "    X_sel = transform.fit_transform(X)\n",
    "    # determine the number of input features\n",
    "    n_features = X_sel.shape[1]\n",
    "    print( ' >Threshold=%.2f, Features=%d ' % (t, n_features))\n",
    "    # store the result\n",
    "    results.append(n_features)\n",
    "# plot the threshold vs the number of selected features\n",
    "pyplot.plot(thresholds, results)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify Rows That Contain Duplicate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "       0    1    2    3               4\n",
      "34   4.9  3.1  1.5  0.1     Iris-setosa\n",
      "37   4.9  3.1  1.5  0.1     Iris-setosa\n",
      "142  5.8  2.7  5.1  1.9  Iris-virginica\n"
     ]
    }
   ],
   "source": [
    "# locate rows of duplicate data\n",
    "from pandas import read_csv\n",
    "# load the dataset\n",
    "df = read_csv( '../../Datasets/iris.csv' , header=None)\n",
    "# calculate duplicates\n",
    "dups = df.duplicated()\n",
    "# report if there are any duplicates\n",
    "print(dups.any())\n",
    "# list all duplicate rows\n",
    "print(df[dups])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 5)\n",
      "(147, 5)\n"
     ]
    }
   ],
   "source": [
    "print(df.shape)\n",
    "# delete duplicate rows\n",
    "df.drop_duplicates(inplace=True)\n",
    "print(df.shape)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "19985ac8a88737d3c8b7fbc1bc9ac2991a55fab1b7ef4317ae756c7f86ac40fa"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
